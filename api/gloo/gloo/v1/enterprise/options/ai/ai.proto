syntax = "proto3";
package ai.options.gloo.solo.io;
option go_package = "github.com/solo-io/solo-apis/pkg/api/gloo.solo.io/v1/enterprise/options/ai";

import "github.com/solo-io/solo-kit/api/v1/ref.proto";
import "google/protobuf/wrappers.proto";
import "google/protobuf/struct.proto";
import "google/protobuf/duration.proto";
import "google/protobuf/empty.proto";
import "extproto/ext.proto";
option (extproto.equal_all) = true;
option (extproto.hash_all) = true;
option (extproto.clone_all) = true;

message SingleAuthToken {
  oneof auth_token_source {
    // Provide easy inline way to specify a token
    string inline = 1;
    // Reference to a secret in the same namespace as the Upstream
    core.solo.io.ResourceRef secret_ref = 2;
  }
}

/*
  The AI UpstreamSpec represents a logical LLM provider backend.
  The purpose of this spec is a way to configure which backend to use
  as well as how to authenticate with the backend.

  Currently the options are:
  - OpenAI
    * Default Host: api.openai.com
    * Default Port: 443
    * Auth Token: Bearer token to use for the OpenAI API
  - Mistral
    * Default Host: api.mistral.com
    * Default Port: 443
    * Auth Token: Bearer token to use for the Mistral API
  - Anthropic
    * Default Host: api.anthropic.com
    * Default Port: 443
    * Auth Token: x-api-key to use for the Anthropic API
    * Version: Optional version header to pass to the Anthropic API

  All of the above backends can be configured to use a custom host and port.
  This option is meant to allow users to proxy the request, or to use a different
  backend altogether which is API compliant with the upstream version.

  Examples:

  OpenAI with inline auth token:
  ```
  ai:
    openai:
      authToken:
        inline: "my_token"
  ```

  Mistral with secret ref:
  ```
  ai:
    mistral:
      authToken:
        secretRef:
          name: "my-secret"
          namespace: "my-ns"
  ```

  Anthropic with inline token and custom Host:
  ```
  ai:
    anthropic:
      authToken:
        inline: "my_token"
      customHost:
        host: "my-anthropic-host.com"
        port: 443 # Port is optional and will default to 443 for HTTPS
  ```
*/
message UpstreamSpec {

  // Settings to configure a custom host to send the traffic to
  message CustomHost {
    // Custom host to send the traffic to
    string host = 1;
    // Custom port to send the traffic to
    uint32 port = 2;
  }

  // Settings for the OpenAI API
  message OpenAI {
    // Auth Token to use for the OpenAI API
    // This token will be placed into the `Authorization` header
    // and prefixed with Bearer if not present
    // when sending the request to the upstream.
    SingleAuthToken auth_token = 1;
    // Optional custom host to send the traffic to
    CustomHost custom_host = 2;
    // Optional: override model name. If not set, the model name will be taken from the request
    // This can be useful when trying model failover scenarios
    // e.g. "gpt-4o-mini"
    string model = 3;
  }

  // Settings for the Azure OpenAI API
  message AzureOpenAI {
    // Auth Token to use for the Azure OpenAI API
    // This token will be placed into the `api-key` header
    oneof auth_token_source {
      // Auth Token to use for the OpenAI API
      // This token will be placed into the `api-key` header
      SingleAuthToken auth_token = 1;
      // use AD or other workload identity mechanism
    }
    
    
    // The endpoint to use
    // This should be the endpoint to the Azure OpenAI API, e.g. my-endpoint.openai.azure.com
    // If the scheme is included it will be stripped.
    // This value can be found https://{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}
    string endpoint = 2;
    // The deployment/model name to use
    // This value can be found https://{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}
    string deployment_name = 3;
    // The version of the API to use
    // This value can be found https://{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}
    string api_version = 4;
  }

  // Settings for the Gemini API
  message Gemini {
    // Auth Token to use for the Gemini API
    // This token will be placed into the `key` header
    oneof auth_token_source {
      // Auth Token to use for the Gemini API
      // This token will be placed into the `key` header
      SingleAuthToken auth_token = 1;
      // TODO: use oauth
    }

    // The model name to use
    // This value can be found https://generativelanguage.googleapis.com/{version}/models/{model}:generateContent?key={api_key}
    string model = 2;
    // The version of the API to use
    // This value can be found https://generativelanguage.googleapis.com/{api_version}/models/{model}:generateContent?key={api_key}
    string api_version = 3;
  }

  // Settings for the Mistral API
  message Mistral {
    // Auth Token to use for the Mistral API.
    // This token will be placed into the `Authorization` header
    // and prefixed with Bearer if not present
    // when sending the request to the upstream.
    SingleAuthToken auth_token = 1;
    // Optional custom host to send the traffic to
    CustomHost custom_host = 2;
    // Optional: override model name. If not set, the model name will be taken from the request
    // This can be useful when trying model failover scenarios
    string model = 3;
  }

  // Settings for the Anthropic API
  message Anthropic {
    // Auth Token to use for the Anthropic API.
    // This token will be placed into the `x-api-key` header
    // when sending the request to the upstream.
    SingleAuthToken auth_token = 1;

    CustomHost custom_host = 2;
    // An optional version header to pass to the Anthropic API
    // See: https://docs.anthropic.com/en/api/versioning for more details
    string version = 3;
    // Optional: override model name. If not set, the model name will be taken from the request
    // This can be useful when trying model failover scenarios
    string model = 4;
  }


  // Composite AI upstream allows you to create a single upstream that
  // is composed of many upstreams. This is useful for creating a single
  // logical endpoint made up of many backends. 
  // The top level list defines the priority of the endpoints, the 2nd 
  // level allows for defining either a list, or a single endpoint for that priority.
  // Note: Only 2 levels of of nesting are allowed, Anything after that will be ignored.
  /*
    multi:
      pools:
      - pool:
        - openai:
            authToken:
              secretRef:
                name: openai-secret
                namespace: gloo-system
        priority: 1
      - pool:
        - azureOpenai:
            deploymentName: gpt-4o-mini
            apiVersion: 2024-02-15-preview
            endpoint: ai-gateway.openai.azure.com
            authToken:
              secretRef:
                name: azure-secret
                namespace: gloo-system
        - azureOpenai:
            deploymentName: gpt-4o-mini-2
            apiVersion: 2024-02-15-preview
            endpoint: ai-gateway.openai.azure.com
            authToken:
              secretRef:
                name: azure-secret
                namespace: gloo-system
        priority: 2
  */
  message MultiPool {
    message Backend {
      oneof llm {
        // OpenAI upstream
        OpenAI openai = 1;
        // Mistral upstream
        Mistral mistral = 2;
        // Anthropic upstream
        Anthropic anthropic = 3;
        // Azure OpenAI upstream
        AzureOpenAI azure_openai = 4;
        // Gemini upstream
        Gemini gemini = 5;
      }
    }

    // Priority represents a single endpoint pool with a given priority
    message Priority {
      // list of backends representing a single endpoint pool
      repeated Backend pool = 1;
    }

    // List of prioritized backend pools
    repeated Priority priorities = 1;
  }

  
  oneof llm {
    // OpenAI upstream
    OpenAI openai = 1;
    // Mistral upstream
    Mistral mistral = 2;
    // Anthropic upstream
    Anthropic anthropic = 3;
    // Azure OpenAI upstream
    AzureOpenAI azure_openai = 4;
    // multi upstream
    MultiPool multi = 5;
    // Gemini upstream
    Gemini gemini = 6;
  }
}

/*
  RouteSettings is a way to configure the behavior of the LLM provider on a per-route basis
  This allows users to configure things like:
  - Prompt Enrichment
  - Retrieval Augmented Generation
  - Semantic Caching
  - Defaults to merge with the user input fields
  - Guardrails
  - Route Type

  NOTE: These settings may only be applied to a route which uses an LLMProvider backend!
*/
message RouteSettings {

  /*
    Config used to enrich the prompt. This can only be used with LLMProviders using the CHAT API type.

    Prompt enrichment allows you to add additional context to the prompt before sending it to the model.
    Unlike RAG or other dynamic context methods, prompt enrichment is static and will be applied to every request.

    Note: Some providers, including Anthropic do not support SYSTEM role messages, but rather have a dedicated
    system field in the input JSON. In this case, `field_defaults` should be used to set the system field. See the docs
    for that field for an example.

    Example:
    ```
    promptEnrichment:
      prepend:
      - role: SYSTEM
        content: "answer all questions in french"
      append:
      - role: USER
        content: "Describe the painting as if you were a famous art critic from the 17th century"
    ```
  */
  AIPromptEnrichment prompt_enrichment = 1;

  /*
    Guards to apply to the LLM requests on this route.
    This can be used to reject requests based on the content of the prompt, as well as
    mask responses based on the content of the response. These guards can be also be used
    at the same time.

    Below is a simple example of a prompt guard that will reject any prompt that contains
    the string "credit card" and will mask any credit card numbers in the response.

    ```
    promptGuard:
      request:
        customResponseMessage: "Rejected due to inappropriate content"
        regex:
          matches:
          - "credit card"
      response:
        regex:
          matches:
          # Mastercard
          - '(?:^|\D)(5[1-5][0-9]{2}(?:\ |\-|)[0-9]{4}(?:\ |\-|)[0-9]{4}(?:\ |\-|)[0-9]{4})(?:\D|$)'
    ````
  */
  AIPromptGuard prompt_guard = 2;

  /*
    Retrieval Augmented Generation. https://research.ibm.com/blog/retrieval-augmented-generation-RAG
    Retrieval Augmented Generation is a process by which you "augment" the information
    a model has access to by providing it with a set of documents to use as context.
    This can be used to improve the quality of the generated text.
    Important Note: The same embedding mechanism must be used for the prompt
    which was used for the initial creation of the context documents.

    Example using postgres for storage and OpenAI for embedding:
    ```
    rag:
      datastore:
        postgres:
          connectionString: postgresql+psycopg://gloo:gloo@172.17.0.1:6024/gloo
          collectionName: default
      embedding:
        openai:
          authToken:
            secretRef:
              name: openai-secret
              namespace: gloo-system
    ```
  */
  RAG rag = 3;

  /*
    Semantic caching configuration
    Semantic caching allows you to cache previous model responses in order to provide
    faster responses to similar requests in the future.
    Results will vary depending on the embedding mechanism used, as well
    as the similarity threshold set.

    Example using Redis for storage and OpenAI for embedding:
    ```
    semanticCache:
      datastore:
        redis:
          connectionString: redis://172.17.0.1:6379
      embedding:
        openai:
          authToken:
            secretRef:
              name: openai-secret
              namespace: gloo-system
    ```
  */
  SemanticCache semantic_cache = 4;

  /*
    A list of defaults to be merged with the user input fields.
    These will NOT override the user input fields unless override is explicitly set to true.
    Some examples include setting the temperature, max_tokens, etc.

    Example overriding system field for Anthropic:
    ```
    # Anthropic doesn't support a system chat type
    defaults:
    - field: "system"
      value: "answer all questions in french"
    ```

    Example setting the temperature and max_tokens, overriding max_tokens:
    ```
    defaults:
    - field: "temperature"
      value: 0.5
    - field: "max_tokens"
      value: 100
    ```
  */
  repeated FieldDefault defaults = 5;

  enum RouteType {
    CHAT = 0;
  }

  // The type of route this is, currently only CHAT is supported
  RouteType route_type = 6;
}

message FieldDefault {
  // Field name
  string field = 1;
  // Field Value, this can be any valid JSON value
  google.protobuf.Value value = 2;
  // Whether or not to override the field if it already exists
  bool override = 3;
}


message Postgres {
  // Connection string to the Postgres database
  string connection_string = 1;
  // Name of the table to use
  string collection_name = 2;
}



message Embedding {

  // OpenAI embedding
  message OpenAI {
    oneof auth_token_source {
      SingleAuthToken auth_token = 1;
      // re-use the token from the backend
      // google.protobuf.Empty inherit_backend_token = 3;
    }
  }

  // Azure OpenAI embedding
  message AzureOpenAI {
    oneof auth_token_source {
      // Auth Token to use for the OpenAI API
      // This token will be placed into the `api-key` header
      SingleAuthToken auth_token = 1;
      // re-use the token from the backend
      // google.protobuf.Empty inherit_backend_token = 3;
    }

    // The version of the API to use
    // This value can be found https://{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}
    string api_version = 2;
    // The endpoint to use
    // This should be the endpoint to the Azure OpenAI API, e.g. https://my-endpoint.openai.azure.com
    // If the scheme isn't included it will be added.
    // This value can be found https://{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}
    string endpoint = 3;
    // The deployment/model name to use
    // This value can be found https://{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}
    string deployment_name = 4;
  }

  oneof embedding {
    // OpenAI embedding
    OpenAI openai = 1;
    // Azure OpenAI embedding
    AzureOpenAI azure_openai = 2;
  }
}

// Settings for the Semantic Caching feature
message SemanticCache {

  // Settings for the Redis database
  message Redis {
    // Connection string to the Redis database
    string connection_string = 1;

    // Similarity score threshold value between 0.0 and 1.0 that determines how similar
    // two queries need to be in order to return a cached result.
    // The lower the number, the more similar the queries need to be for a cache hit.
    //
    // +kubebuilder:validation:Minimum=0
    // +kubebuilder:validation:Maximum=1
    float score_threshold = 2;
  }

  // Settings for the Weaviate database
  message Weaviate {
    // Connection string to the Weaviate database, scheme should NOT be included.
    // For example: weaviate.my-ns.svc.cluster.local
    // NOT: http://weaviate.my-ns.svc.cluster.local
    string host = 1;
    // HTTP port to use, if unset will default to 8080
    uint32 http_port = 2;
    // GRPC port to use, if unset will default to 50051
    uint32 grpc_port = 3;
    // Whether or not to use a secure connection, true by default
    bool insecure = 4;
  }

  // Data store from which to cache the request/response pairs
  message DataStore {
    oneof datastore {
      Redis redis = 1;
      Weaviate weaviate = 2;
    }
  }
  enum Mode {
    // Read and write to the cache as a part of the request/response lifecycle
    READ_WRITE = 0;
    // Only read from the cache, do not write to it. Data will be written to the cache outside the request/response cycle.
    READ_ONLY = 1;
  }
  // Which data store to use
  DataStore datastore = 1;
  // Model to use to get embeddings for prompt
  Embedding embedding = 2;
  // Time before data in the cache is considered expired
  uint32 ttl = 3;
  // Cache mode to use: READ_WRITE or READ_ONLY
  Mode mode = 4;
}

// Settings for the Retrieval Augmented Generation feature
message RAG {
  message DataStore {
    oneof datastore {
      Postgres postgres = 1;
    }
  }
  // Data store from which to fetch the embeddings
  DataStore datastore = 1;
  // Model to use to get embeddings for prompt
  Embedding embedding = 2;
  // Template to use to embed the returned context
  string prompt_template = 3;
}

// Settings for the Prompt Enrichment feature
message AIPromptEnrichment {
  message Message {
    // Role of the message.
    // The available roles depend on the backend model being used,
    // please consult the documentation for more information.
    string role = 1;
    // String content of the message
    string content = 2;
  }
  // A list of messages to be prepended to the prompt sent by the client
  repeated Message prepend = 2;
  // A list of messages to be appended to the prompt sent by the client
  repeated Message append = 3;

}

// Settings for the Prompt Guard feature
message AIPromptGuard {

  // Regex settings for prompt guard
  message Regex {
    enum BuiltIn {
      // Default REGEX for Social Security Numbers
      SSN = 0;
      // Default REGEX for Credit Card Numbers
      CREDIT_CARD = 1;
    }
    // A list of Regex patterns to match against the response.
    // All matches will be masked before being sent back to the client.
    // matches and builtins are additive.
    repeated string matches = 1;
    // A list of built-in regexes to mask in the response.
    // matches and builtins are additive.
    repeated BuiltIn builtins = 2;
  }

  // Webhook settings for prompt guard
  message Webhook {
    // Host to send the traffic to.
    string host = 1;
    // Port to send the traffic to
    uint32 port = 2;
    message HeaderMatch {
      enum MatchType {
        // Exact match
        EXACT = 0;
        // Prefix match
        PREFIX = 1;
        // Suffix match
        SUFFIX = 2;
        // Contains match
        CONTAINS = 3;
        // Regex match
        REGEX = 4;
      }
      // Header key to match
      string key = 1;
      // Type of match to use
      MatchType match_type = 2;
    }
    // Headers to forward with the request
    repeated HeaderMatch headers = 3;
  }


  // Request settings for Prompt Guard
  message Request {
    message CustomResponse {
      // Custom response message to send back to the client.
      // If not specified, the following default message will be used:
      // "The request was rejected due to inappropriate content"
      string message = 1;

      // Status code to send back to the client.
      uint32 status_code = 2;
    }
    // Custom response message to send back to the client.
    // If not specified, the following default message will be used:
    // "The request was rejected due to inappropriate content"
    CustomResponse custom_response = 1;

    // Regex request guard
    Regex regex = 2;

    // Webhook request guard
    Webhook webhook = 3;
  }

  // Request settings for Prompt Guard
  message Response {
    // Regex response guard
    Regex regex = 1;

    // Webhook response guard
    Webhook webhook = 2;
  }
  // Guards for the prompt request
  Request request = 1;
  // Guards for the LLM response
  Response response = 2;
}